{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "gdT3OJiV2kU5"
      },
      "outputs": [],
      "source": [
        "# Utility.py File\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import seaborn as sns\n",
        "import torch.distributions as dist\n",
        "from mpl_toolkits import mplot3d\n",
        "from torch.distributions.dirichlet import Dirichlet\n",
        "\n",
        "from scipy.stats import multivariate_normal\n",
        "from scipy.stats import norm\n",
        "\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import sys\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from scipy.stats import multivariate_normal\n",
        "from scipy.stats import norm\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import sys\n",
        "\n",
        "import torch.distributions as dist\n",
        "from torch.distributions.dirichlet import Dirichlet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from scipy.stats import multivariate_normal\n",
        "from scipy.stats import norm\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import sys\n",
        "\n",
        "import torch.distributions as dist\n",
        "from torch.distributions.dirichlet import Dirichlet\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "r46pM1SQ2y8R"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_dir(alpha, size):\n",
        "  model = Dirichlet(torch.tensor(alpha))\n",
        "  sample = model.sample(torch.Size([size])).data\n",
        "  fig = plt.figure()\n",
        "  ax = plt.axes(projection='3d')\n",
        "  ax.scatter3D(sample[:, 0], sample[:, 1], sample[:, 2], color='red')\n",
        "  ax.plot([0, 0], [1, 0], [0, 1], linewidth=3, color='purple')\n",
        "  ax.plot([0, 1], [0, 0], [1, 0], linewidth=3, color='purple')\n",
        "  ax.plot([0, 1], [1, 0], [0, 0], linewidth=3, color='purple')\n",
        "  ax.set_xlim((0, 1))\n",
        "  ax.set_ylim((0, 1))\n",
        "  ax.set_zlim((0, 1))\n",
        "  ax.view_init(60, 35)\n",
        "\n",
        "\n",
        "def numpy_to_tensor(arr):\n",
        "  ret = torch.Tensor([arr])\n",
        "  return ret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "uEvCxWbi3IEX"
      },
      "outputs": [],
      "source": [
        "def target_alpha(targets):\n",
        "  target = targets.numpy()\n",
        "  def gen_onehot(category, total_cat=10):\n",
        "    label = np.ones(total_cat)\n",
        "    label[category] = 20\n",
        "    return label\n",
        "  target_alphas = []\n",
        "  for i in target:\n",
        "    if i==10:\n",
        "      target_alphas.append(torch.tensor(np.ones(10)))\n",
        "    else:\n",
        "      target_alphas.append(torch.tensor(gen_onehot(i)))\n",
        "  return torch.stack(target_alphas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "xJDsz9GZ3RF8"
      },
      "outputs": [],
      "source": [
        "class PriorNet(nn.Module):\n",
        "    def __init__(self, input_dim=1, hidden_dim=1):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, 3)         # output_dim = 4\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x #F.softmax(x, dim=1)\n",
        "\n",
        "    def predict_alpha(self, x):\n",
        "      src = torch.Tensor([x]).to(device)\n",
        "      output = torch.exp(self.forward(src))\n",
        "      return output\n",
        "\n",
        "    def predict_dir(self, x):\n",
        "      alpha = self.predict_alpha(x)\n",
        "      dirichlet = Dirichlet(alpha)\n",
        "      return dirichlet\n",
        "\n",
        "    def fit(self, epoch_num, optimizer, train_X, train_Y):\n",
        "      self.train()\n",
        "\n",
        "      n_train = len(train_X)\n",
        "\n",
        "      # Shuffle the input\n",
        "      index = np.arange(n_train)\n",
        "      np.random.shuffle(index)\n",
        "      train_x = train_X[index]\n",
        "      train_y = train_Y[index]\n",
        "\n",
        "      for epoch in range(epoch_num):\n",
        "        for i in range(n_train):\n",
        "          optimizer.zero_grad()\n",
        "          src = torch.Tensor(train_x[i:i+1]).to(device)\n",
        "          target = torch.Tensor(train_y[i:i+1]).to(device)\n",
        "          # Predicted alpha\n",
        "          output = torch.exp(self.forward(src))\n",
        "          dirichlet1 = Dirichlet(output)\n",
        "          dirichlet2 = Dirichlet(target)\n",
        "          loss = dist.kl.kl_divergence(dirichlet1, dirichlet2)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "        print('Train Epoch: {} \\t Loss: {:.6f}'.format(epoch, loss.item()))\n",
        "    \n",
        "    def expected_entropy_from_alphas(self, alphas, alpha0):\n",
        "        return -torch.sum((alphas/alpha0)*(torch.digamma(alphas+1)-torch.digamma(alpha0+1)), dim=1)\n",
        "    \n",
        "    def categorical_entropy_torch(self, probs):\n",
        "        log_probs = torch.log(probs)\n",
        "        log_probs = torch.where(torch.isfinite(log_probs), log_probs, torch.zeros_like(log_probs))\n",
        "        entropy = -torch.sum(probs*log_probs, dim=1, keepdim=False)\n",
        "        return entropy\n",
        "\n",
        "    def mutual_information(self, x):\n",
        "        \n",
        "        alphas = self.predict_alpha(x)\n",
        "        alpha0 = torch.sum(alphas, dim=1, keepdim=True)\n",
        "        probs = alphas / alpha0\n",
        "\n",
        "        expected_entropy = self.expected_entropy_from_alphas(alphas, alpha0)\n",
        "        entropy_of_exp = self.categorical_entropy_torch(probs)\n",
        "        mutual_info = entropy_of_exp - expected_entropy\n",
        "        return mutual_info\n",
        "\n",
        "    def diffenrential_entropy(self, x):\n",
        "        alphas = self.predict_alpha(x)\n",
        "        alpha0 = torch.sum(alphas, dim=1, keepdim=True)\n",
        "        return torch.sum(\n",
        "            torch.lgamma(alphas)-(alphas-1)*(torch.digamma(alphas)-torch.digamma(alpha0)),\n",
        "            dim=1) - torch.lgamma(alpha0)\n",
        "\n",
        "    def entropy(self, x):\n",
        "        alphas = self.predict_alpha(x)\n",
        "        alpha0 = torch.sum(alphas, dim=1, keepdim=True)\n",
        "        probs = alphas / alpha0\n",
        "        ret = -torch.sum(probs*torch.log(probs), dim=1)\n",
        "        return ret\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "7c5wtChz3XHF"
      },
      "outputs": [],
      "source": [
        "class PriorNet_CNN(PriorNet):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(16000, 50)\n",
        "        self.fc2 = nn.Linear(50, 20)\n",
        "        self.fc3 = nn.Linear(20, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 16000)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def fit(self, epoch_num, optimizer, train_all):\n",
        "      self.train()\n",
        "\n",
        "      # Shuffle the input\n",
        "      train_loader = torch.utils.data.DataLoader(train_all, \n",
        "                                                 batch_size=32, \n",
        "                                                 shuffle=True)\n",
        "\n",
        "      for epoch in range(epoch_num):\n",
        "        loss_total = 0\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          data = data.to(device)\n",
        "\n",
        "          # predict alpha\n",
        "          target_a = target_alpha(target)\n",
        "          target_a = target_a.to(device)\n",
        "          output_alpha = torch.exp(self.forward(data))\n",
        "          dirichlet1 = Dirichlet(output_alpha)\n",
        "          dirichlet2 = Dirichlet(target_a)\n",
        "\n",
        "          loss = torch.sum(dist.kl.kl_divergence(dirichlet1, dirichlet2))\n",
        "          loss_total += loss.item()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "        print('Train Epoch: {} \\t Loss: {:.6f}'.format(epoch, loss_total/120000))\n",
        "    \n",
        "    def max_prob(self, x):\n",
        "        alphas = self.predict_alpha(x)\n",
        "        alpha0 = torch.sum(alphas, dim=1, keepdim=True)\n",
        "        probs = alphas / alpha0\n",
        "        conf = torch.max(probs, dim=1)\n",
        "        return conf.values\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cifar_mean = (0.4914, 0.4822, 0.4465)\n",
        "cifar_std = (0.2470, 0.2435, 0.2616)                                             \n",
        "cifar_transform =  transforms.Compose(\n",
        "      [transforms.ToTensor(),\n",
        "     transforms.Normalize(cifar_mean, cifar_std)])"
      ],
      "metadata": {
        "id": "QCAzYkCzOJDb"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "lFL48Sr73m5A"
      },
      "outputs": [],
      "source": [
        "class DNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(20 * 5 * 5, 50)\n",
        "        self.fc2 = nn.Linear(50, 20)\n",
        "        self.fc3 = nn.Linear(20, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        # print(x.shape)\n",
        "        x = x.view(-1, 20 * 5 * 5)\n",
        "        # print(x.shape)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)\n",
        "        # print(x.shape)\n",
        "        return F.log_softmax(x)\n",
        "\n",
        "    def fit(self, optimizer, epoch):\n",
        "      train_in = torchvision.datasets.CIFAR10('/files/', train=True, download=True,\n",
        "                                      transform=cifar_transform)\n",
        "      train_loader = torch.utils.data.DataLoader(train_in, batch_size=32, shuffle=True)\n",
        "      self.train()\n",
        "      for epoch_num in range(epoch):\n",
        "        loss_total = 0\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          data = data.to(device)\n",
        "          target = target.to(device)\n",
        "            \n",
        "          output = self.forward(data)\n",
        "          loss = F.nll_loss(output, target)\n",
        "          loss_total += loss.item()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "        print('Train Epoch: {} \\t Loss: {:.6f}'.format(epoch_num, loss_total/120000))\n",
        "        \n",
        "\n",
        "    def test(self):\n",
        "      self.eval()\n",
        "      test_loss = 0\n",
        "      correct = 0\n",
        "      test_in = torchvision.datasets.CIFAR10('/files/', train=False, download=True,\n",
        "                                        transform=cifar_transform)\n",
        "      # print(test_in.data.shape)\n",
        "      test_loader = torch.utils.data.DataLoader(test_in, batch_size=32, shuffle=True)\n",
        "      with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "\n",
        "          data = data.to(device)\n",
        "          target = target.to(device)\n",
        "          output = self.forward(data)\n",
        "          # print(output)\n",
        "          # print(target)\n",
        "          # print(data.shape)\n",
        "          # print(output.shape)\n",
        "          # print(target.shape)\n",
        "          test_loss += F.nll_loss(output, target, size_average=False).item()\n",
        "          pred = output.data.max(1, keepdim=True)[1]\n",
        "          correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "      test_loss /= len(test_loader.dataset)\n",
        "\n",
        "      print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "          test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset))\n",
        "      )\n",
        "\n",
        "    def predict_alpha(self, x):\n",
        "      self.eval()\n",
        "      src = torch.Tensor([x]).to(device)\n",
        "      output = torch.exp(self.forward(src))\n",
        "      return output\n",
        "\n",
        "    def max_prob(self, x):\n",
        "        alphas = self.predict_alpha(x)\n",
        "        alpha0 = torch.sum(alphas, dim=1, keepdim=True)\n",
        "        probs = alphas / alpha0\n",
        "        conf = torch.max(probs, dim=1)\n",
        "        return conf.values\n",
        "\n",
        "    def entropy(self, x):\n",
        "        alphas = self.predict_alpha(x)\n",
        "        alpha0 = torch.sum(alphas, dim=1, keepdim=True)\n",
        "        probs = alphas / alpha0\n",
        "        ret = -torch.sum(probs*torch.log(probs), dim=1)\n",
        "        return ret\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "s3zZapU731sI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21c0bd76-875d-4853-ec40-1194a7ff43d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Using downloaded and verified file: ./data/train_32x32.mat\n",
            "Using downloaded and verified file: ./data/test_32x32.mat\n"
          ]
        }
      ],
      "source": [
        "train_in = torchvision.datasets.CIFAR10('/files/', train=True, download=True,\n",
        "                                        transform=cifar_transform)\n",
        "\n",
        "test_in = torchvision.datasets.CIFAR10('/files/', train=False, download=True,\n",
        "                                        transform=cifar_transform)\n",
        "train_out = torchvision.datasets.SVHN('./data', split ='train', download=True,\n",
        "                                        transform=cifar_transform)\n",
        "test_out = torchvision.datasets.SVHN('./data', split ='test', download=True,\n",
        "                                        transform=cifar_transform)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(torch.from_numpy(train_out.data).shape)\n",
        "# print(torch.from_numpy(train_in.data).shape)\n",
        "\n",
        "# print(torch.from_numpy(test_out.data).shape)\n",
        "# print(torch.from_numpy(test_in.data).shape)"
      ],
      "metadata": {
        "id": "qpSLDRTYd4f4"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_out.data = np.vstack(train_in.data).reshape(-1, 32, 32, 3)\n",
        "test_out.data = np.vstack(test_in.data).reshape(-1, 32, 32, 3)\n",
        "# print(torch.from_numpy(train_out.data).shape)\n",
        "# print(torch.from_numpy(train_in.data).shape)\n",
        "\n",
        "# print(torch.from_numpy(test_out.data).shape)\n",
        "# print(torch.from_numpy(test_in.data).shape)"
      ],
      "metadata": {
        "id": "fN2hndecOyC8"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_in.data = torch.from_numpy(train_in.data)\n",
        "train_out.data = torch.from_numpy(train_out.data)\n",
        "\n",
        "# train_in.targets = torch.Tensor(train_in.targets)\n",
        "# train_out.labels = torch.Tensor(train_out.labels)"
      ],
      "metadata": {
        "id": "BNawK5OTdEVJ"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(torch.IntTensor(train_in.targets))\n",
        "# print(torch.Tensor(train_out.labels))"
      ],
      "metadata": {
        "id": "91gt7UyqrpAf"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "CU_uTXTl36wm"
      },
      "outputs": [],
      "source": [
        "train_out.targets = torch.tensor(np.ones(len(train_out.labels))*10, dtype=torch.long)\n",
        "test_out.targets = torch.tensor(np.ones(len(test_out.labels))*10, dtype=torch.long)\n",
        "\n",
        "train_all = train_in\n",
        "train_all.data = torch.cat((train_in.data, train_out.data))\n",
        "train_all.targets = torch.cat((torch.IntTensor(np.array(train_in.targets)), torch.IntTensor(np.array(train_out.labels))))\n",
        "train_all.data = train_all.data.cpu().detach().numpy()\n",
        "train_all.targets = train_all.targets.data.cpu().detach().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEftRGVB3_EY",
        "outputId": "d2670dd2-7259-40d8-805f-7388da39a429"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0 \t Loss: 27.884020\n",
            "Train Epoch: 1 \t Loss: 27.570767\n",
            "Train Epoch: 2 \t Loss: 27.566496\n",
            "Train Epoch: 3 \t Loss: 27.563733\n",
            "Train Epoch: 4 \t Loss: 27.563529\n",
            "Train Epoch: 5 \t Loss: 27.563505\n",
            "Train Epoch: 6 \t Loss: 27.563496\n",
            "Train Epoch: 7 \t Loss: 27.559312\n",
            "Train Epoch: 8 \t Loss: 27.560968\n",
            "Train Epoch: 9 \t Loss: 27.560303\n",
            "Train Epoch: 10 \t Loss: 27.560191\n",
            "Train Epoch: 11 \t Loss: 27.557952\n",
            "Train Epoch: 12 \t Loss: 27.557607\n",
            "Train Epoch: 13 \t Loss: 27.559588\n",
            "Train Epoch: 14 \t Loss: 27.557566\n",
            "Train Epoch: 15 \t Loss: 27.555918\n",
            "Train Epoch: 16 \t Loss: 27.557064\n",
            "Train Epoch: 17 \t Loss: 27.558242\n",
            "Train Epoch: 18 \t Loss: 27.556257\n",
            "Train Epoch: 19 \t Loss: 27.555600\n",
            "Train Epoch: 20 \t Loss: 27.556680\n",
            "Train Epoch: 21 \t Loss: 27.553397\n",
            "Train Epoch: 22 \t Loss: 27.554868\n",
            "Train Epoch: 23 \t Loss: 27.553563\n",
            "Train Epoch: 24 \t Loss: 27.554151\n",
            "Train Epoch: 25 \t Loss: 27.553167\n",
            "Train Epoch: 26 \t Loss: 27.554359\n",
            "Train Epoch: 27 \t Loss: 27.553187\n",
            "Train Epoch: 28 \t Loss: 27.555186\n",
            "Train Epoch: 29 \t Loss: 27.553964\n"
          ]
        }
      ],
      "source": [
        "dpn = PriorNet_CNN()\n",
        "if torch.cuda.is_available(): dpn.cuda()\n",
        "optimizer = optim.Adam(dpn.parameters(), lr=0.001)\n",
        "\n",
        "dpn.fit(30, optimizer, train_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alDv3Rt_6-Zg",
        "outputId": "4bc3ddc7-413f-4e6d-ace1-7e66662c7b42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Avg. loss: 2.3048, Accuracy: 1009/10000 (10%)\n",
            "\n",
            "Files already downloaded and verified\n",
            "Train Epoch: 0 \t Loss: 0.024385\n",
            "Train Epoch: 1 \t Loss: 0.022006\n",
            "Train Epoch: 2 \t Loss: 0.021179\n",
            "Train Epoch: 3 \t Loss: 0.020694\n",
            "Train Epoch: 4 \t Loss: 0.020307\n",
            "Train Epoch: 5 \t Loss: 0.020049\n",
            "Train Epoch: 6 \t Loss: 0.019794\n",
            "Train Epoch: 7 \t Loss: 0.019677\n",
            "Train Epoch: 8 \t Loss: 0.019512\n",
            "Train Epoch: 9 \t Loss: 0.019362\n",
            "Train Epoch: 10 \t Loss: 0.019241\n",
            "Train Epoch: 11 \t Loss: 0.019138\n",
            "Train Epoch: 12 \t Loss: 0.019038\n",
            "Train Epoch: 13 \t Loss: 0.018969\n",
            "Train Epoch: 14 \t Loss: 0.018882\n",
            "Train Epoch: 15 \t Loss: 0.018775\n",
            "Train Epoch: 16 \t Loss: 0.018795\n",
            "Train Epoch: 17 \t Loss: 0.018759\n",
            "Train Epoch: 18 \t Loss: 0.018630\n",
            "Train Epoch: 19 \t Loss: 0.018598\n",
            "Train Epoch: 20 \t Loss: 0.018585\n",
            "Train Epoch: 21 \t Loss: 0.018468\n",
            "Train Epoch: 22 \t Loss: 0.018468\n",
            "Train Epoch: 23 \t Loss: 0.018440\n",
            "Train Epoch: 24 \t Loss: 0.018408\n",
            "Train Epoch: 25 \t Loss: 0.018344\n",
            "Train Epoch: 26 \t Loss: 0.018323\n",
            "Train Epoch: 27 \t Loss: 0.018353\n",
            "Train Epoch: 28 \t Loss: 0.018265\n",
            "Train Epoch: 29 \t Loss: 0.018294\n",
            "Files already downloaded and verified\n",
            "\n",
            "Test set: Avg. loss: 1.1850, Accuracy: 5935/10000 (59%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dnn = DNN()\n",
        "if torch.cuda.is_available(): dnn.cuda()\n",
        "optimizer = optim.Adam(dnn.parameters(), lr=0.001)\n",
        "\n",
        "dnn.test()\n",
        "dnn.fit(optimizer, 30)\n",
        "dnn.test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "Jq3k9nph4IKa"
      },
      "outputs": [],
      "source": [
        "# miscood.py\n",
        "\n",
        "def uncertainty_score(model, test_data, metrics):\n",
        "  model.eval()\n",
        "  score_lst = []\n",
        "  for x in test_data:\n",
        "    x = [x]\n",
        "    with torch.no_grad():\n",
        "      if metrics=='DE':\n",
        "        score = model.diffenrential_entropy(x).data.cpu().numpy()[0][0]\n",
        "      elif metrics=='MI':\n",
        "        score = model.mutual_information(x).data.cpu().numpy()[0]\n",
        "      elif metrics=='MAXP':\n",
        "        score = model.max_prob(x).data.cpu().numpy()[0]\n",
        "      elif metrics=='ENT':\n",
        "        score = model.entropy(x).data.cpu().numpy()[0]\n",
        "    score_lst.append(score)\n",
        "  return score_lst\n",
        "\n",
        "\n",
        "def get_ood_label_score(test_in_score, test_out_score):\n",
        "  score = np.concatenate([test_in_score, test_out_score])\n",
        "  label = np.concatenate((np.zeros(len(test_in_score)), np.ones(len(test_out_score))))\n",
        "  return label, score\n",
        "  \n",
        "def get_misc_label_score(model, test_data, test_label, test_in_score):\n",
        "  misc_label = np.zeros(len(test_label))\n",
        "\n",
        "  for i in range(len(test_data)):\n",
        "    x = test_data[i]\n",
        "    x = [x]\n",
        "    with torch.no_grad():\n",
        "      pred = model.predict_alpha(x)\n",
        "      pred_class = torch.max(pred[0], 0).indices\n",
        "      misc_label[i] = 1-torch.eq(test_label[i], pred_class).sum().cpu().numpy()\n",
        "\n",
        "  return misc_label, np.array(test_in_score)\n",
        "\n",
        "def plot_roc(label, score, label_name):\n",
        "  fpr, tpr, thresholds = roc_curve(label, score)\n",
        "  plt.plot(fpr, tpr, label=label_name)\n",
        "  plt.xlabel('False Positive')\n",
        "  plt.ylabel('True Positive')\n",
        "  plt.title('ROC')\n",
        "  plt.ylim(0.0, 1.0)\n",
        "  plt.xlim(0.0, 1.0)\n",
        "\n",
        "def plot_pr(label, score, label_name):\n",
        "  precision, recall, thresholds = precision_recall_curve(label, score)\n",
        "  plt.plot(recall, precision, label=label_name)\n",
        "  plt.xlabel('Recall')\n",
        "  plt.ylabel('Precision')\n",
        "  plt.title('Precision-Recall Curve')\n",
        "  plt.ylim(0.0, 1.0)\n",
        "  plt.xlim(0.0, 1.0)\n",
        "\n",
        "\n",
        "def get_auroc_aupr(label, score):\n",
        "  #score[score==-np.inf] = -100\n",
        "  #score[score==np.inf] = 100\n",
        "\n",
        "  auroc = roc_auc_score(label, score)\n",
        "  precision, recall, thresholds = precision_recall_curve(label, score)\n",
        "  aupr = auc(recall, precision)\n",
        "  return auroc, aupr\n",
        "\n",
        "\n",
        "def get_test_roc_pr(network, metrics, detect='ood'):\n",
        "  test_in_score = uncertainty_score(network, test_in.data.numpy(), metrics)\n",
        "  test_out_score = uncertainty_score(network, test_out.data.numpy(), metrics)\n",
        "  if detect=='ood':\n",
        "    label_dp, score_dp = get_ood_label_score(test_in_score, test_out_score)\n",
        "  elif detect=='misc':\n",
        "    label_dp, score_dp = get_misc_label_score(network, test_in.data.numpy(),\n",
        "                                              test_in.targets, test_in_score)\n",
        "  if metrics=='MAXP':\n",
        "    score_dp = -score_dp\n",
        "\n",
        "  index = np.isposinf(score_dp)\n",
        "  score_dp[np.isposinf(score_dp)] = 1e9\n",
        "  maximum = np.amax(score_dp)\n",
        "  score_dp[np.isposinf(score_dp)] = maximum + 1\n",
        "\n",
        "  index = np.isneginf(score_dp)\n",
        "  score_dp[np.isneginf(score_dp)] = -1e9\n",
        "  minimum = np.amin(score_dp)\n",
        "  score_dp[np.isneginf(score_dp)] = minimum - 1\n",
        "\n",
        "  score_dp[np.isnan(score_dp)] = 0\n",
        "\n",
        "  auroc, aupr = get_auroc_aupr(label_dp, score_dp)\n",
        "  return auroc, aupr, label_dp, score_dp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "oS8E0fjl44Cs",
        "outputId": "faa82a6d-4891-4704-95a4-6b54c27ef6c5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-b1f0220f60ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mauroc_mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maupr_mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_test_roc_pr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdpn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MAXP'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'misc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mauroc_ent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maupr_ent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_ent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_ent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_test_roc_pr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdpn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ENT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'misc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mauroc_mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maupr_mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_test_roc_pr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdpn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MI'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'misc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mauroc_de\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maupr_de\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_de\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_de\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_test_roc_pr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdpn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'misc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-70-33abe9f541bc>\u001b[0m in \u001b[0;36mget_test_roc_pr\u001b[0;34m(network, metrics, detect)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_test_roc_pr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ood'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m   \u001b[0mtest_in_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muncertainty_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m   \u001b[0mtest_out_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muncertainty_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdetect\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'ood'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'numpy'"
          ]
        }
      ],
      "source": [
        "auroc_mp, aupr_mp, label_mp, score_mp = get_test_roc_pr(dpn, 'MAXP', 'misc')\n",
        "auroc_ent, aupr_ent, label_ent, score_ent = get_test_roc_pr(dpn, 'ENT', 'misc')\n",
        "auroc_mi, aupr_mi, label_mi, score_mi = get_test_roc_pr(dpn, 'MI', 'misc')\n",
        "auroc_de, aupr_de, label_de, score_de = get_test_roc_pr(dpn, 'DE', 'misc')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyvPA0n-7eOv"
      },
      "outputs": [],
      "source": [
        "auroc_lst = [auroc_mp, auroc_ent, auroc_mi, auroc_de]\n",
        "aupr_lst = [aupr_mp, aupr_ent, aupr_mi, aupr_de]\n",
        "\n",
        "pd.DataFrame(data=np.array([auroc_lst, aupr_lst]), \n",
        "             columns=['Max.P', 'Ent', 'MI', 'D.Ent'], \n",
        "             index=['AUROC', 'AUPR'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_aBCtjY4wV_"
      },
      "source": [
        "ROC curve for DPN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPwtYoxm4qjc"
      },
      "outputs": [],
      "source": [
        "plot_roc(label_mp, score_mp, label_name='MAX.P')\n",
        "plot_roc(label_ent, score_ent, label_name='ENT')\n",
        "plot_roc(label_mi, score_mi, label_name='MI')\n",
        "plot_roc(label_de, score_de, label_name='D.ENT')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaR118vX4ylg"
      },
      "source": [
        "Precision-Recall for DPN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jz6yKSS94vph"
      },
      "outputs": [],
      "source": [
        "plot_pr(label_mp, score_mp, label_name='MAX.P')\n",
        "plot_pr(label_ent, score_ent, label_name='ENT')\n",
        "plot_pr(label_mi, score_mi, label_name='MI')\n",
        "plot_pr(label_de, score_de, label_name='D.ENT')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4NEI2Vf720b"
      },
      "source": [
        "**DNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxM8UCfa716Y"
      },
      "outputs": [],
      "source": [
        "auroc_mp, aupr_mp, _, _ = get_test_roc_pr(dnn, 'MAXP', 'misc')\n",
        "auroc_ent, aupr_ent, _, _ = get_test_roc_pr(dnn, 'ENT', 'misc')\n",
        "\n",
        "auroc_lst = [auroc_mp, auroc_ent]\n",
        "aupr_lst = [aupr_mp, aupr_ent]\n",
        "\n",
        "pd.DataFrame(data=np.array([auroc_lst, aupr_lst]), \n",
        "             columns=['Max.P', 'Ent'], \n",
        "             index=['AUROC', 'AUPR'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "Hv6aM3kL9FFy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8e08002-182b-4232-abc0-d1eb30cef349"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "model_save_name_dpn = 'dpn_cifar.pt'\n",
        "path_dpn = F\"/content/drive/MyDrive/{model_save_name_dpn}\" \n",
        "# torch.save(dpn.state_dict(), path_dpn)\n",
        "\n",
        "model_save_name_dnn = 'dnn_cifar.pt'\n",
        "path_dnn = F\"/content/drive/MyDrive/{model_save_name_dnn}\" \n",
        "torch.save(dnn.state_dict(), path_dnn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "GTPm7EK9-hbL"
      },
      "outputs": [],
      "source": [
        "# dpn.save('/content/drive/MyDrive/dpn_entire.pt')\n",
        "torch.save(dnn, '/content/drive/MyDrive/dnn_entire_cifar.pt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6RljHiswnS2_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}